# XGBoost rządzi 

[Vladimir Alekseichenko, GE Healthcare](http://dataworkshop.eu)

## Opis warsztatu 

W teorii nie ma różnicy pomiędzy teorią i praktyką. W praktyce - jest. XGBoost to jest najlepsza implementacja "Gradient Boosting" z punktu widzenia praktycznego. 

Z kilku powodów: 
1. wynik (czyli zwykle jeden z najlepszych)
2. czas na naukę i predykcję (potrafi używać wszystkie dostępne rdzenie)
3. odporność na przeuczenia się (poprzez różne parametry)
4. stabilność (można spokojnie używać na produkcje)

## Plan warsztatu 

1. Wprowadzenie do Gradient Boosting (oraz kilka innych tematów "overfitting" i "model evaluation")
2. Rozwiązanie problemu używając na XGBoost
3. Zaawansowane rozwiązanie używając XGBoost

## Wymagane pakiety 



XGBoost (najważniejsze i najtrudniejszy pakiet do instalowania), resztę muszę przemyśleć

## Wymagane od uczestników umiejętności i wiedza 

Wydaję się, że ten warsztat może być ciekawy dla osób które dopiero zaczynają, jak i dla średnio-zaawansowanych (z mojej wiedzy mało osób kojarzy i tym bardziej używa XGBoost w praktyce)

## Wymagania wstępne do wykonania przed warsztatem 

Dobre pytanie, ale wymaga czasu na przygotowanie się. 

Myślę o trzech rzeczach:
1. Mieć laptop z potrzebnymi pakietami (przede wszystkim xgboost)
2. Pomyśleć nad problemem przed warsztatem (może nawet spróbować go rozwiązać w najlepszy możliwy sposób - model, który się zna)
3. Pobrać dane i źródła (natomiast jeżeli wybiorę przypadek z mniejszą ilością danych, to ten krok może być wykonany tuż przed warsztatem)

## Język warsztatu 

polski